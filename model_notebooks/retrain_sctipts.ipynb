{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPhlGAqkPImz6dCj/C9jW4/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XQZ1mDUC1T2s","executionInfo":{"status":"ok","timestamp":1692820545657,"user_tz":-120,"elapsed":7,"user":{"displayName":"Данило Сушко","userId":"16602607562590349620"}},"outputId":"9e561455-3b54-4a6d-ea33-f928844d71d5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Writing retrain_functions.py\n"]}],"source":["%%writefile retrain_functions.py\n","import torch\n","from torch.utils.data import Dataset\n","from torchvision import transforms\n","from torch.utils.data import DataLoader\n","from torchvision import datasets\n","\n","from tqdm.auto import tqdm\n","from typing import Dict, List, Tuple\n","\n","import numpy as np\n","\n","def train_test_transforms(image_size=224):\n","  \"\"\"\n","  The function to create transforms for train and test datasets.\n","\n","  Args: image_size: int | Risize image to (image_size X image_size) | Default: 224\n","\n","  Return: Tuple(train_transformer, test_transformer)\n","  \"\"\"\n","\n","  # Create transforms for train data\n","  train_transformer = transforms.Compose([\n","      transforms.ToPILImage(),\n","      transforms.Resize(image_size),\n","      transforms.CenterCrop(image_size),\n","      transforms.RandomChoice( [\n","                                transforms.RandomHorizontalFlip(p=0.5),\n","                                transforms.ColorJitter(contrast=0.9),\n","                                transforms.ColorJitter(brightness=0.1),\n","                                transforms.RandomApply( [ transforms.RandomHorizontalFlip(p=1), transforms.ColorJitter(contrast=0.9) ], p=0.5),\n","                                transforms.RandomApply( [ transforms.RandomHorizontalFlip(p=1), transforms.ColorJitter(brightness=0.1) ], p=0.5),\n","                                ] ),\n","      transforms.ToTensor(),\n","      transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","  ])\n","\n","  # Create transforms for test data\n","  test_transformer = transforms.Compose([\n","          transforms.ToPILImage(),\n","          transforms.Resize(image_size),\n","          transforms.CenterCrop(image_size),\n","          transforms.ToTensor(),\n","          transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","      ])\n","\n","  return train_transformer, test_transformer\n","\n","def create_dataloaders(train_arr, test_arr, train_classes, test_classes, train_transformer, test_transformer, batch_size=4):\n","  \"\"\"\n","  The function to create DataLoaders.\n","\n","  Args: train_arr: numpy.array | NumPy array of train images\n","        test_arr: numpy.array | NumPy array of test images\n","        train_classes: numpy.array | NumPy array of train class names\n","        test_classes: numpy.array | NumPy array of test class names\n","        train_transformer: torchvision.transforms | Train transforms\n","        test_transformer: torchvision.transforms | Test transforms\n","        batch_size: int | Batch size | Default: 4\n","\n","  Return: Tuple(train_dataloader, test_dataloader, train_data, test_data, class_names)\n","\n","  train_dataloader: Train DataLoader\n","  test_dataloader: Test DataLoader\n","  train_data: Train Dataset\n","  test_data: Test Dataset\n","  class_names: Class names\n","  \"\"\"\n","\n","  class CustomDataset(Dataset):\n","    def __init__(self, images, class_names, transform=None):\n","        self.images = images\n","        self.class_names = class_names\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.images)\n","\n","    def __getitem__(self, idx):\n","        image = self.images[idx]\n","        label = self.class_names[idx]\n","\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        return image, label\n","\n","\n","  # Create Datasets\n","  train_data = CustomDataset(train_arr, train_classes, transform = train_transformer)\n","  test_data = CustomDataset(test_arr, test_classes, transform = test_transformer)\n","\n","  # Get class names\n","  class_names = list(set(train_data.class_names).union(set(test_data.class_names)))\n","\n","  # Create train DataLoader for train data\n","  train_dataloader = DataLoader(dataset=train_data,\n","                                batch_size=batch_size,\n","                                shuffle=True)\n","\n","  # Create train DataLoader for train data\n","  test_dataloader = DataLoader(dataset=test_data,\n","                                batch_size=batch_size,\n","                                shuffle=False)\n","\n","  return train_dataloader, test_dataloader, train_data, test_data, class_names\n","\n","def train_step(model: torch.nn.Module,\n","               dataloader: torch.utils.data.DataLoader,\n","               loss_fn: torch.nn.Module,\n","               optimizer: torch.optim.Optimizer,\n","               device: torch.device) -> Tuple[float, float]:\n","    \"\"\"Trains a PyTorch model for a single epoch.\n","\n","    Turns a target PyTorch model to training mode and then\n","    runs through all of the required training steps (forward\n","    pass, loss calculation, optimizer step).\n","\n","    Args:\n","    model: A PyTorch model to be trained.\n","    dataloader: A DataLoader instance for the model to be trained on.\n","    loss_fn: A PyTorch loss function to minimize.\n","    optimizer: A PyTorch optimizer to help minimize the loss function.\n","    device: A target device to compute on (e.g. \"cpu\").\n","\n","    Returns:\n","    A tuple of training loss and training accuracy metrics.\n","    In the form (train_loss, train_accuracy). For example:\n","\n","    (0.1112, 0.8743)\n","    \"\"\"\n","    # Put model in train mode\n","    model.train()\n","\n","    # Setup train loss and train accuracy values\n","    train_loss, train_acc = 0, 0\n","\n","    # Loop through data loader data batches\n","    for batch, (X, y) in enumerate(dataloader):\n","        # Send data to target device\n","        X, y = X.to(device), y.to(device)\n","\n","        # 1. Forward pass\n","        y_pred = model(X)\n","\n","        # 2. Calculate  and accumulate loss\n","        loss = loss_fn(y_pred, y)\n","        train_loss += loss.item()\n","\n","        # 3. Optimizer zero grad\n","        optimizer.zero_grad()\n","\n","        # 4. Loss backward\n","        loss.backward()\n","\n","        # 5. Optimizer step\n","        optimizer.step()\n","\n","        # Calculate and accumulate accuracy metric across all batches\n","        y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n","        train_acc += (y_pred_class == y).sum().item()/len(y_pred)\n","\n","    # Adjust metrics to get average loss and accuracy per batch\n","    train_loss = train_loss / len(dataloader)\n","    train_acc = train_acc / len(dataloader)\n","    return train_loss, train_acc\n","\n","def test_step(model: torch.nn.Module,\n","              dataloader: torch.utils.data.DataLoader,\n","              loss_fn: torch.nn.Module,\n","              device: torch.device) -> Tuple[float, float]:\n","    \"\"\"Tests a PyTorch model for a single epoch.\n","\n","    Turns a target PyTorch model to \"eval\" mode and then performs\n","    a forward pass on a testing dataset.\n","\n","    Args:\n","    model: A PyTorch model to be tested.\n","    dataloader: A DataLoader instance for the model to be tested on.\n","    loss_fn: A PyTorch loss function to calculate loss on the test data.\n","    device: A target device to compute on (e.g. \"cpu\").\n","\n","    Returns:\n","    A tuple of testing loss and testing accuracy metrics.\n","    In the form (test_loss, test_accuracy). For example:\n","\n","    (0.0223, 0.8985)\n","    \"\"\"\n","    # Put model in eval mode\n","    model.eval()\n","\n","    # Setup test loss and test accuracy values\n","    test_loss, test_acc = 0, 0\n","\n","    # Turn on inference context manager\n","    with torch.inference_mode():\n","        # Loop through DataLoader batches\n","        for batch, (X, y) in enumerate(dataloader):\n","            # Send data to target device\n","            X, y = X.to(device), y.to(device)\n","\n","            # 1. Forward pass\n","            test_pred_logits = model(X)\n","\n","            # 2. Calculate and accumulate loss\n","            loss = loss_fn(test_pred_logits, y)\n","            test_loss += loss.item()\n","\n","            # Calculate and accumulate accuracy\n","            test_pred_labels = test_pred_logits.argmax(dim=1)\n","            test_acc += ((test_pred_labels == y).sum().item()/len(test_pred_labels))\n","\n","    # Adjust metrics to get average loss and accuracy per batch\n","    test_loss = test_loss / len(dataloader)\n","    test_acc = test_acc / len(dataloader)\n","    return test_loss, test_acc\n","\n","def train(model: torch.nn.Module,\n","          train_dataloader: torch.utils.data.DataLoader,\n","          test_dataloader: torch.utils.data.DataLoader,\n","          optimizer: torch.optim.Optimizer,\n","          loss_fn: torch.nn.Module,\n","          scheduler,\n","          early_stopper,\n","          epochs: int,\n","          device: torch.device) -> Dict[str, List]:\n","    \"\"\"Trains and tests a PyTorch model.\n","\n","    Passes a target PyTorch models through train_step() and test_step()\n","    functions for a number of epochs, training and testing the model\n","    in the same epoch loop.\n","\n","    Calculates, prints and stores evaluation metrics throughout.\n","\n","    Args:\n","    model: A PyTorch model to be trained and tested.\n","    train_dataloader: A DataLoader instance for the model to be trained on.\n","    test_dataloader: A DataLoader instance for the model to be tested on.\n","    optimizer: A PyTorch optimizer to help minimize the loss function.\n","    loss_fn: A PyTorch loss function to calculate loss on both datasets.\n","    epochs: An integer indicating how many epochs to train for.\n","    device: A target device to compute on (e.g. \"cpu\").\n","\n","    Returns:\n","    A dictionary of training and testing loss as well as training and\n","    testing accuracy metrics. Each metric has a value in a list for\n","    each epoch.\n","    In the form: {train_loss: [...],\n","              train_acc: [...],\n","              test_loss: [...],\n","              test_acc: [...]}\n","    For example if training for epochs=2:\n","             {train_loss: [2.0616, 1.0537],\n","              train_acc: [0.3945, 0.3945],\n","              test_loss: [1.2641, 1.5706],\n","              test_acc: [0.3400, 0.2973]}\n","    \"\"\"\n","    # Create empty results dictionary\n","    results = {\"train_loss\": [],\n","               \"train_acc\": [],\n","               \"test_loss\": [],\n","               \"test_acc\": []\n","    }\n","\n","    model.to(device)\n","\n","    # Loop through training and testing steps for a number of epochs\n","    for epoch in tqdm(range(epochs)):\n","        train_loss, train_acc = train_step(model=model,\n","                                          dataloader=train_dataloader,\n","                                          loss_fn=loss_fn,\n","                                          optimizer=optimizer,\n","                                          device=device)\n","\n","        test_loss, test_acc = test_step(model=model,\n","                                        dataloader=test_dataloader,\n","                                        loss_fn=loss_fn,\n","                                        device=device)\n","\n","        scheduler.step()\n","\n","        # Print out what's happening\n","        print(\n","          f\"Epoch: {epoch+1} | \"\n","          f\"train_loss: {train_loss:.4f} | \"\n","          f\"train_acc: {train_acc:.4f} | \"\n","          f\"test_loss: {test_loss:.4f} | \"\n","          f\"test_acc: {test_acc:.4f}\"\n","        )\n","\n","        # Update results dictionary\n","        results[\"train_loss\"].append(train_loss)\n","        results[\"train_acc\"].append(train_acc)\n","        results[\"test_loss\"].append(test_loss)\n","        results[\"test_acc\"].append(test_acc)\n","\n","        if early_stopper.early_stop(test_loss):\n","          return results\n","    # Return the filled results at the end of the epochs\n","    return results"]},{"cell_type":"code","source":["%%writefile utils.py\n","import torch\n","import random\n","\n","def set_seeds(seed=42):\n","  \"\"\"\n","    Set random seeds (random.seed and torch.manual_seed)\n","  \"\"\"\n","  random.seed(seed)\n","  torch.manual_seed(42)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_c7tr0v116HI","executionInfo":{"status":"ok","timestamp":1692820547929,"user_tz":-120,"elapsed":511,"user":{"displayName":"Данило Сушко","userId":"16602607562590349620"}},"outputId":"a56fd33a-3f67-44cb-bf7f-56b921967979"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing utils.py\n"]}]},{"cell_type":"code","source":["%%writefile early_stopper.py\n","\"\"\"\n"," Class EarlyStopper to stop training if validation loss increase.\n","\"\"\"\n","\n","class EarlyStopper:\n","    def __init__(self, patience=1, min_delta=0):\n","        self.patience = patience\n","        self.min_delta = min_delta\n","        self.counter = 0\n","        self.min_validation_loss = np.inf\n","\n","    def early_stop(self, validation_loss):\n","        if validation_loss < self.min_validation_loss:\n","            self.min_validation_loss = validation_loss\n","            self.counter = 0\n","        elif validation_loss > (self.min_validation_loss + self.min_delta):\n","            self.counter += 1\n","            if self.counter >= self.patience:\n","                return True\n","        return False"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kM5UXStnCS0v","executionInfo":{"status":"ok","timestamp":1692820548451,"user_tz":-120,"elapsed":10,"user":{"displayName":"Данило Сушко","userId":"16602607562590349620"}},"outputId":"b6843333-38de-4dd9-ae61-dc475152cd73"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing early_stopper.py\n"]}]},{"cell_type":"code","source":["%%writefile retrain.py\n","import torch\n","import torch.nn as nn\n","from torch.optim import Adam\n","from torch.optim.lr_scheduler import StepLR\n","from sklearn.model_selection import train_test_split\n","\n","from model import create_mobilenet\n","from utils import set_seeds\n","from retrain_functions import *\n","from early_stopper import EarlyStopper\n","\n","import numpy as np\n","import os\n","\n","cwd = os.getcwd()\n","\n","def numpy_to_tensor(array):\n","  \"\"\"\n","    Convert numpy array to tensor.\n","  \"\"\"\n","  preprocessed_images = []\n","  for image in array[:,0]:\n","      # Convert to numpy array and normalize pixel values\n","      image_array = np.array(image) / 255.0\n","\n","      # Transpose the dimensions to (channels, height, width)\n","      image_array = np.transpose(image_array, (2, 0, 1))\n","\n","      preprocessed_images.append(image_array)\n","\n","  # Stack the preprocessed images into a tensor\n","  image_tensor = torch.tensor(preprocessed_images)\n","\n","  return image_tensor\n","\n","def build_and_retrain_model(images, class_idx, new_test):\n","  \"\"\"The function, that retrain a model.\n","\n","  Args:\n","    images: numpy.array\n","    class_idx: Dict()\n","    new_test: numpy.array\n","\n","  Return:\n","    model.state_dict(): Tensor()\n","    model_results: Dict(train_loss, test_loss, train_acc, test_acc)\n","  \"\"\"\n","  # Constants\n","  RANDOM_SEED = 42\n","  IMAGE_SIZE = 224\n","  BATCH_SIZE = 4\n","  TEST_SIZE = 0.2\n","  NUM_OF_CLASSES = 4\n","\n","  # Set random seeds\n","  set_seeds(RANDOM_SEED)\n","\n","  train_arr, test_arr = train_test_split(images, test_size=TEST_SIZE, random_state=RANDOM_SEED, stratify=images[:,1])\n","\n","  # Add previous test files to test_arr\n","  test_arr = np.array([test_arr, new_test])\n","\n","  train_classes = train_arr[:,1]\n","  test_classes = test_arr[:,1]\n","\n","  def class_name_to_digit(names, class_idx):\n","    result = []\n","    for name in names:\n","      result.append(class_idx.get(name))\n","    return np.array(result)\n","\n","  train_classes_idx = torch.from_numpy(class_name_to_digit(train_classes, class_idx))\n","  test_classes_idx = torch.from_numpy(class_name_to_digit(test_classes, class_idx))\n","\n","  # Create transforms\n","  train_transforms, test_transforms = train_test_transforms(image_size=IMAGE_SIZE)\n","\n","  # Numpy to tensor\n","  train_tensor, test_tensor = numpy_to_tensor(train_arr), numpy_to_tensor(test_arr)\n","\n","  # Create DataLoaders\n","  train_dataloader, test_dataloader, train_data, test_data, class_names = create_dataloaders(train_arr=train_tensor,\n","                                                                                            test_arr=test_tensor,\n","                                                                                            train_classes=train_classes_idx,\n","                                                                                            test_classes=test_classes_idx,\n","                                                                                            train_transformer=train_transforms,\n","                                                                                            test_transformer=test_transforms,\n","                                                                                            batch_size=BATCH_SIZE)\n","  # Create the model and optimizer\n","  model, transformer = create_mobilenet(NUM_OF_CLASSES)\n","  model.load_state_dict(\n","    torch.load(f=os.path.join(cwd,\"models/predict/simpsons_model.pth\"),\n","               map_location=torch.device(\"cpu\"))\n","  )\n","\n","  optimizer = Adam(params=model.parameters(), lr=0.001, weight_decay=1e-5)\n","\n","  # Use learning rate scheduler\n","  scheduler = StepLR(optimizer=optimizer,\n","                    step_size=3,\n","                    gamma=0.1,\n","                    last_epoch=-1,\n","                    verbose=True)\n","\n","  # Use label smoothing in the loss function\n","  loss_fn = nn.CrossEntropyLoss(label_smoothing=0.1)\n","\n","  # Set early stopper to prevent overfitting\n","  early_stopper = EarlyStopper(patience=3, min_delta=0.1)\n","\n","  # Train the model\n","  model_results = train(model=model,\n","                        train_dataloader=train_dataloader,\n","                        test_dataloader=test_dataloader,\n","                        optimizer=optimizer,\n","                        loss_fn=loss_fn,\n","                        scheduler=scheduler,\n","                        early_stopper=early_stopper,\n","                        epochs=10,\n","                        device='cpu')\n","\n","  return model.state_dict(), model_results"],"metadata":{"id":"nyaHJV-y1XU8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692820548451,"user_tz":-120,"elapsed":10,"user":{"displayName":"Данило Сушко","userId":"16602607562590349620"}},"outputId":"8429c79d-2b0f-4c98-a720-acf16f506604"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing retrain.py\n"]}]},{"cell_type":"code","source":["from PIL import Image\n","from pathlib import Path\n","import numpy as np\n","\n","def get_images(path):\n","  full_array = []\n","  for one_path in Path(path).glob('*/'):\n","    image = Image.open(one_path).resize((224,224))\n","    full_array.append((np.asarray(image), 'Bart Simpson'))\n","\n","  return np.array(full_array)\n","\n","images = get_images('test')\n","images[0]"],"metadata":{"id":"PYOZHO_kp2m3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import retrain\n","import importlib\n","importlib.reload(retrain)\n","\n","new_state_dict, new_model_results = retrain.build_and_retrain_model(images)\n","\n","new_model_results"],"metadata":{"id":"eKRcCAJBp4EB"},"execution_count":null,"outputs":[]}]}